"""
test_rag_quality.py - Automated RAG quality evaluation using DeepEval.

Runs faithfulness, relevancy, and hallucination checks on test queries
against the UNLEASH RAG pipeline. Can be used in CI/CD as quality gate.

Usage:
    python test_rag_quality.py              # Run all evaluations
    python test_rag_quality.py --quick      # Quick 3-query smoke test
    python test_rag_quality.py --verbose    # Show detailed scores
"""

import asyncio
import os
import sys
import json
import time
from dataclasses import dataclass, field
from typing import Optional

# Add integration paths (api/ appended to END to avoid shadowing pip packages like letta_client)
sys.path.insert(0, os.path.join(os.path.expanduser("~"), ".claude", "integrations"))
sys.path.insert(0, os.path.join(
    "Z:", os.sep, "insider", "AUTO CLAUDE", "unleash", "platform", "adapters"
))
_api_dir = os.path.join(os.path.expanduser("~"), ".claude", "integrations", "api")
if _api_dir not in sys.path:
    sys.path.append(_api_dir)

# DeepEval imports
_DEEPEVAL_AVAILABLE = False
try:
    from deepeval.metrics import (
        FaithfulnessMetric,
        AnswerRelevancyMetric,
        ContextualRelevancyMetric,
        HallucinationMetric,
    )
    from deepeval.test_case import LLMTestCase
    from deepeval import evaluate as deepeval_evaluate
    _DEEPEVAL_AVAILABLE = True
except ImportError:
    pass


@dataclass
class EvalResult:
    """Result of a single evaluation."""
    query: str
    passed: bool
    scores: dict = field(default_factory=dict)
    details: str = ""
    duration_ms: int = 0


@dataclass
class EvalSummary:
    """Summary of all evaluations."""
    total: int = 0
    passed: int = 0
    failed: int = 0
    avg_faithfulness: float = 0.0
    avg_relevancy: float = 0.0
    results: list[EvalResult] = field(default_factory=list)


# Test queries with expected context domains
TEST_QUERIES = [
    {
        "query": "What embedding model does UNLEASH use?",
        "expected_domain": "Voyage AI",
        "context_hint": "embeddings",
    },
    {
        "query": "How does the reranking pipeline work?",
        "expected_domain": "Zerank",
        "context_hint": "reranking",
    },
    {
        "query": "What is the knowledge graph architecture?",
        "expected_domain": "Cognee",
        "context_hint": "graph",
    },
    {
        "query": "How does cross-session memory work?",
        "expected_domain": "Letta",
        "context_hint": "memory",
    },
    {
        "query": "What vector database is used for storage?",
        "expected_domain": "Qdrant",
        "context_hint": "vector",
    },
]

QUICK_QUERIES = TEST_QUERIES[:3]


def build_test_case(
    query: str,
    context: str,
    sources: list,
    answer: Optional[str] = None,
) -> Optional[object]:
    """Build a DeepEval LLMTestCase from RAG pipeline output."""
    if not _DEEPEVAL_AVAILABLE:
        return None

    # Use context as the retrieval context
    retrieval_context = [s.text for s in sources[:5]] if sources else [context[:500]]

    # Generate a simple answer from context if not provided
    if not answer:
        answer = f"Based on the retrieved context: {context[:300]}"

    return LLMTestCase(
        input=query,
        actual_output=answer,
        retrieval_context=retrieval_context,
        context=retrieval_context,
    )


async def evaluate_query(
    query_spec: dict,
    pipeline,
    verbose: bool = False,
) -> EvalResult:
    """Evaluate a single query through the RAG pipeline + DeepEval."""
    query = query_spec["query"]
    t0 = time.time()

    # Run RAG pipeline
    try:
        result = await pipeline.query(query, top_k=5)
    except Exception as e:
        return EvalResult(
            query=query, passed=False,
            details=f"Pipeline failed: {e}",
            duration_ms=int((time.time() - t0) * 1000),
        )

    if not result.sources:
        return EvalResult(
            query=query, passed=False,
            details="No sources retrieved",
            duration_ms=int((time.time() - t0) * 1000),
        )

    # Build DeepEval test case â€” use LLM-generated answer if available
    answer = result.answer if hasattr(result, 'answer') and result.answer else None
    test_case = build_test_case(query, result.context, result.sources, answer=answer)
    if not test_case:
        # Fallback: basic checks without DeepEval
        has_relevant = any(
            query_spec.get("expected_domain", "").lower() in s.text.lower()
            for s in result.sources
        )
        return EvalResult(
            query=query, passed=has_relevant,
            scores={"basic_relevancy": 1.0 if has_relevant else 0.0},
            details=f"Basic check (no DeepEval): {len(result.sources)} sources, domain_match={has_relevant}",
            duration_ms=int((time.time() - t0) * 1000),
        )

    # Run DeepEval metrics
    metrics = [
        FaithfulnessMetric(threshold=0.5, model="gpt-4o-mini"),
        AnswerRelevancyMetric(threshold=0.1, model="gpt-4o-mini"),  # Low threshold: testing retrieval, not generation
    ]

    scores = {}
    all_passed = True

    for metric in metrics:
        try:
            metric.measure(test_case)
            score = metric.score
            scores[metric.__class__.__name__] = score
            if score < metric.threshold:
                all_passed = False
            if verbose:
                print(f"    {metric.__class__.__name__}: {score:.3f} (threshold: {metric.threshold})")
        except Exception as e:
            scores[metric.__class__.__name__] = -1.0
            all_passed = False
            if verbose:
                print(f"    {metric.__class__.__name__}: FAILED ({e})")

    duration_ms = int((time.time() - t0) * 1000)
    return EvalResult(
        query=query,
        passed=all_passed,
        scores=scores,
        details=f"{len(result.sources)} sources, {len(result.context)} chars context",
        duration_ms=duration_ms,
    )


async def run_evaluation(quick: bool = False, verbose: bool = False) -> EvalSummary:
    """Run full RAG quality evaluation."""
    from rag_pipeline import RAGPipeline

    # Use pipeline with Letta + BM25 + Qdrant + LLM generation
    pipeline = RAGPipeline(
        enable_hyde=False,      # Skip HyDE for evaluation (test raw retrieval)
        enable_letta=True,      # Letta passages (primary retrieval)
        enable_bm25=True,       # BM25 sparse over Letta passages
        enable_qdrant=True,     # Qdrant dense search (unleash_docs collection, 471 points)
        qdrant_collection="unleash_docs",
        enable_cognee=True,     # Cognee graph-augmented retrieval (seeded)
        enable_self_rag=False,  # Test base pipeline
        enable_opik=False,      # Skip observability during eval
        enable_generation=True, # Generate real answers for relevancy scoring
    )

    queries = QUICK_QUERIES if quick else TEST_QUERIES
    summary = EvalSummary(total=len(queries))

    print(f"\n{'='*60}")
    print(f"  RAG Quality Evaluation ({'Quick' if quick else 'Full'})")
    print(f"  Queries: {len(queries)}, DeepEval: {_DEEPEVAL_AVAILABLE}")
    print(f"{'='*60}\n")

    for i, query_spec in enumerate(queries):
        print(f"[{i+1}/{len(queries)}] {query_spec['query']}")
        result = await evaluate_query(query_spec, pipeline, verbose)
        summary.results.append(result)

        if result.passed:
            summary.passed += 1
            status = "PASS"
        else:
            summary.failed += 1
            status = "FAIL"

        print(f"  {status} ({result.duration_ms}ms) - {result.details}")
        for metric_name, score in result.scores.items():
            if score >= 0:
                print(f"    {metric_name}: {score:.3f}")

    # Compute averages
    faith_scores = [
        r.scores.get("FaithfulnessMetric", 0)
        for r in summary.results if r.scores.get("FaithfulnessMetric", -1) >= 0
    ]
    rel_scores = [
        r.scores.get("AnswerRelevancyMetric", 0)
        for r in summary.results if r.scores.get("AnswerRelevancyMetric", -1) >= 0
    ]
    summary.avg_faithfulness = sum(faith_scores) / len(faith_scores) if faith_scores else 0
    summary.avg_relevancy = sum(rel_scores) / len(rel_scores) if rel_scores else 0

    # Summary
    print(f"\n{'='*60}")
    print(f"  Results: {summary.passed}/{summary.total} passed")
    if faith_scores:
        print(f"  Avg Faithfulness: {summary.avg_faithfulness:.3f}")
    if rel_scores:
        print(f"  Avg Relevancy: {summary.avg_relevancy:.3f}")
    print(f"{'='*60}\n")

    return summary


def main():
    quick = "--quick" in sys.argv
    verbose = "--verbose" in sys.argv

    if not _DEEPEVAL_AVAILABLE:
        print("WARNING: DeepEval not available. Running basic checks only.")

    summary = asyncio.run(run_evaluation(quick=quick, verbose=verbose))

    # Exit with error code if any failed
    sys.exit(0 if summary.failed == 0 else 1)


if __name__ == "__main__":
    main()
