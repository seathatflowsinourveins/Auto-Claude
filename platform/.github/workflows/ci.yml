# Ultimate Autonomous Platform - CI/CD Pipeline
# Runs tests, linting, and performance benchmarks on push and PR
#
# Triggers:
#   - Push to main/develop branches
#   - Pull requests to main
#   - Manual workflow dispatch

name: Platform CI/CD

on:
  push:
    branches: [main, develop]
    paths:
      - 'v10_optimized/**'
      - '.github/workflows/ci.yml'
  pull_request:
    branches: [main]
    paths:
      - 'v10_optimized/**'
  workflow_dispatch:
    inputs:
      run_benchmarks:
        description: 'Run performance benchmarks'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  QDRANT_URL: 'http://localhost:6333'
  NEO4J_URI: 'bolt://localhost:7687'
  NEO4J_AUTH: 'neo4j/testpassword'

jobs:
  # ===========================================================================
  # LINT - Code Quality Checks
  # ===========================================================================
  lint:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install linting tools
        run: |
          pip install ruff mypy

      - name: Run Ruff linter
        run: |
          ruff check v10_optimized/ --output-format=github
        continue-on-error: true

      - name: Run type checking
        run: |
          mypy v10_optimized/ --ignore-missing-imports --no-error-summary
        continue-on-error: true

  # ===========================================================================
  # TEST - Unit and Integration Tests
  # ===========================================================================
  test:
    name: Tests
    runs-on: ubuntu-latest
    needs: lint

    services:
      # Qdrant service for vector database tests
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
        options: >-
          --health-cmd "wget -q --spider http://localhost:6333/collections || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Neo4j service for graph database tests
      neo4j:
        image: neo4j:5.15.0
        ports:
          - 7474:7474
          - 7687:7687
        env:
          NEO4J_AUTH: neo4j/testpassword
        options: >-
          --health-cmd "wget -q --spider http://localhost:7474 || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 30s

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install httpx structlog qdrant-client neo4j rich pytest pytest-asyncio

      - name: Wait for services
        run: |
          echo "Waiting for Qdrant..."
          timeout 60 bash -c 'until curl -s http://localhost:6333/collections; do sleep 2; done'
          echo "Waiting for Neo4j..."
          timeout 90 bash -c 'until curl -s http://localhost:7474; do sleep 2; done'

      - name: Run Swarm Coordinator Tests
        run: |
          cd v10_optimized/swarm
          python test_coordinator.py
        env:
          QDRANT_URL: ${{ env.QDRANT_URL }}

      - name: Run Live Qdrant Tests
        run: |
          cd v10_optimized/swarm
          python test_live_qdrant.py
        env:
          QDRANT_URL: ${{ env.QDRANT_URL }}

      - name: Run Parallel Execution Tests
        run: |
          cd v10_optimized/swarm
          python test_parallel_agents.py --workers 3 --tasks 15
        env:
          QDRANT_URL: ${{ env.QDRANT_URL }}

  # ===========================================================================
  # BENCHMARK - Performance Tests (Optional)
  # ===========================================================================
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: test
    if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.run_benchmarks == 'true' }}

    services:
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install httpx structlog qdrant-client

      - name: Wait for Qdrant
        run: |
          timeout 60 bash -c 'until curl -s http://localhost:6333/collections; do sleep 2; done'

      - name: Run Benchmarks
        run: |
          cd v10_optimized/swarm
          python benchmark_coordinator.py

      - name: Run Scaling Tests
        run: |
          cd v10_optimized/swarm
          python test_parallel_agents.py --scaling

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: v10_optimized/swarm/*.log
          retention-days: 30
        if: always()

  # ===========================================================================
  # BUILD - Docker Image Build (for releases)
  # ===========================================================================
  build:
    name: Docker Build
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Verify Docker Compose
        run: |
          docker-compose -f v10_optimized/deploy/docker-compose.yml config

      - name: Build test
        run: |
          echo "Docker Compose configuration validated successfully"
          echo "Production build would proceed here"

  # ===========================================================================
  # SUMMARY - Final Status Report
  # ===========================================================================
  summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: always()

    steps:
      - name: Check job results
        run: |
          echo "============================================================"
          echo "  CI/CD Pipeline Summary"
          echo "============================================================"
          echo "  Lint: ${{ needs.lint.result }}"
          echo "  Test: ${{ needs.test.result }}"
          echo "============================================================"

      - name: Fail if tests failed
        if: needs.test.result == 'failure'
        run: exit 1
