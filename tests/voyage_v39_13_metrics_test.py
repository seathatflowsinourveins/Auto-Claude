#!/usr/bin/env python3
"""
Voyage AI V39.13 - Classification Enhancement & Metrics Tests

REAL API TESTS - These make actual Voyage AI API calls.
Validates classification metrics, thresholds, transitions, and adaptive smoothing.

Run with: python tests/voyage_v39_13_metrics_test.py
"""

import asyncio
import os
import sys
import time
from typing import Any

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Load environment
from dotenv import load_dotenv
env_path = os.path.join(project_root, ".config", ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)
    print(f"[INFO] Loaded environment from {env_path}")

from core.orchestration.embedding_layer import (
    EmbeddingLayer,
    EmbeddingConfig,
    EmbeddingModel,
    # V39.12 exports (required for V39.13)
    ARCHETYPE_NAMES,
    ArchetypeEmbeddingLibrary,
    StreamingClassificationResult,
    StreamingClassifier,
    # V39.13 exports
    ClassificationMetrics,
    ClassificationThresholds,
    TransitionEvent,
    TransitionDetector,
    AdaptiveSmoother,
    EnhancedClassificationResult,
    EnhancedStreamingClassifier,
)


# Test configuration
TEST_RESULTS: dict[str, Any] = {
    "passed": 0,
    "failed": 0,
    "skipped": 0,
    "total_tokens": 0,
    "total_cost": 0.0,
}


def estimate_cost(tokens: int, model: str = "voyage-4-large") -> float:
    """Estimate API cost for tokens used."""
    rates = {
        "voyage-4-large": 0.03 / 1_000_000,
        "voyage-4-lite": 0.01 / 1_000_000,
    }
    rate = rates.get(model, 0.03 / 1_000_000)
    return tokens * rate


async def run_test(name: str, test_func, *args, **kwargs) -> bool:
    """Run a single test with timing and cost tracking."""
    print(f"\n[TEST] {name}")
    print("-" * 50)

    start_time = time.time()
    try:
        result = await test_func(*args, **kwargs)
        elapsed = time.time() - start_time

        tokens = result.get("tokens", 0) if isinstance(result, dict) else 0
        cost = estimate_cost(tokens)
        TEST_RESULTS["total_tokens"] += tokens
        TEST_RESULTS["total_cost"] += cost

        print(f"  [PASS] {name} ({elapsed:.2f}s, ~${cost:.4f})")
        TEST_RESULTS["passed"] += 1
        return True
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"  [FAIL] {name} ({elapsed:.2f}s)")
        print(f"    Error: {e}")
        import traceback
        traceback.print_exc()
        TEST_RESULTS["failed"] += 1
        return False


# =============================================================================
# V39.13 Phase 1: Core Infrastructure Tests
# =============================================================================

async def test_classification_metrics_dataclass():
    """Test ClassificationMetrics dataclass initialization and recording."""
    metrics = ClassificationMetrics()

    # Test default values
    assert metrics.total_classifications == 0
    assert metrics.high_confidence_count == 0
    assert metrics.low_confidence_count == 0
    assert metrics.transitions_detected == 0
    assert metrics.avg_confidence == 0.0
    assert metrics.avg_stability == 0.0
    assert metrics.avg_latency_ms == 0.0
    assert len(metrics.category_distribution) == 0

    # Create a mock result for recording
    mock_result = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"WARRIOR": 0.6, "SAGE": 0.3, "JESTER": 0.1},
        smoothed_probabilities={"WARRIOR": 0.55, "SAGE": 0.35, "JESTER": 0.1},
        dominant_category="WARRIOR",
        dominant_confidence=0.55,
        temporal_stability=0.85,
        window_size=5,
    )

    # Record the result (high_confidence determined internally: 0.55 < 0.7 threshold)
    metrics.record(mock_result, latency_ms=15.5)

    assert metrics.total_classifications == 1
    assert metrics.high_confidence_count == 0  # 0.55 < 0.7 threshold
    assert metrics.low_confidence_count == 1
    assert abs(metrics.avg_confidence - 0.55) < 0.001
    assert abs(metrics.avg_stability - 0.85) < 0.001
    assert abs(metrics.avg_latency_ms - 15.5) < 0.001
    assert metrics.category_distribution["WARRIOR"] == 1

    print(f"  Total classifications: {metrics.total_classifications}")
    print(f"  Avg confidence: {metrics.avg_confidence:.2%}")
    print(f"  Category distribution: {metrics.category_distribution}")
    return {"tokens": 0}


async def test_classification_metrics_running_averages():
    """Test that ClassificationMetrics computes running averages correctly."""
    metrics = ClassificationMetrics()

    # Record multiple results
    results_data = [
        (0.5, 0.8, 10.0, "WARRIOR"),
        (0.7, 0.85, 12.0, "WARRIOR"),
        (0.6, 0.9, 8.0, "SAGE"),
        (0.8, 0.75, 15.0, "SAGE"),
        (0.4, 0.7, 11.0, "JESTER"),
    ]

    for conf, stab, lat, cat in results_data:
        mock_result = StreamingClassificationResult(
            timestamp=time.time(),
            raw_probabilities={cat: conf},
            smoothed_probabilities={cat: conf},
            dominant_category=cat,
            dominant_confidence=conf,
            temporal_stability=stab,
            window_size=5,
        )
        # high_confidence is determined internally (conf >= 0.7 threshold)
        metrics.record(mock_result, latency_ms=lat)

    # Verify counts
    assert metrics.total_classifications == 5
    assert metrics.high_confidence_count == 2  # 0.7 and 0.8
    assert metrics.low_confidence_count == 3   # 0.5, 0.6, 0.4

    # Verify running averages
    expected_avg_conf = (0.5 + 0.7 + 0.6 + 0.8 + 0.4) / 5
    expected_avg_stab = (0.8 + 0.85 + 0.9 + 0.75 + 0.7) / 5
    expected_avg_lat = (10.0 + 12.0 + 8.0 + 15.0 + 11.0) / 5

    assert abs(metrics.avg_confidence - expected_avg_conf) < 0.001
    assert abs(metrics.avg_stability - expected_avg_stab) < 0.001
    assert abs(metrics.avg_latency_ms - expected_avg_lat) < 0.001

    # Verify distribution
    assert metrics.category_distribution["WARRIOR"] == 2
    assert metrics.category_distribution["SAGE"] == 2
    assert metrics.category_distribution["JESTER"] == 1

    print(f"  Running averages validated: conf={metrics.avg_confidence:.2%}, stab={metrics.avg_stability:.2%}")
    print(f"  Distribution: {metrics.category_distribution}")
    return {"tokens": 0}


async def test_classification_metrics_serialization():
    """Test ClassificationMetrics to_dict serialization."""
    metrics = ClassificationMetrics()

    # Record some data
    for i in range(10):
        mock_result = StreamingClassificationResult(
            timestamp=time.time(),
            raw_probabilities={"WARRIOR": 0.6},
            smoothed_probabilities={"WARRIOR": 0.6},
            dominant_category="WARRIOR",
            dominant_confidence=0.6,
            temporal_stability=0.8,
            window_size=5,
        )
        metrics.record(mock_result, latency_ms=12.0)

    # Add transitions via record method (recreate mock_result for pyright)
    transition_result = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"SAGE": 0.7},
        smoothed_probabilities={"SAGE": 0.7},
        dominant_category="SAGE",
        dominant_confidence=0.7,  # >= 0.7 threshold for variety
        temporal_stability=0.8,
        window_size=5,
    )
    metrics.record(transition_result, latency_ms=12.0, is_transition=True)
    metrics.record(transition_result, latency_ms=12.0, is_transition=True)

    # Serialize (now has 12 total, 2 transitions)
    data = metrics.to_dict()

    assert "total" in data
    assert "high_confidence_rate" in data
    assert "high_confidence_count" in data
    assert "transition_rate" in data
    assert "avg_confidence" in data
    assert "avg_stability" in data
    assert "avg_latency_ms" in data
    assert "distribution" in data

    assert data["total"] == 12  # 10 from loop + 2 transitions
    assert abs(data["transition_rate"] - 2/12) < 0.001  # 2/12 â‰ˆ 0.1667

    print(f"  Serialized fields: {list(data.keys())}")
    print(f"  Total: {data['total']}, Transition rate: {data['transition_rate']:.1%}")
    return {"tokens": 0}


async def test_classification_thresholds():
    """Test ClassificationThresholds validation logic."""
    thresholds = ClassificationThresholds(
        min_confidence=0.3,
        high_confidence=0.7,
        stability_weight=0.3,
    )

    # Create results at different confidence levels
    low_conf_result = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"WARRIOR": 0.2},
        smoothed_probabilities={"WARRIOR": 0.2},
        dominant_category="WARRIOR",
        dominant_confidence=0.2,
        temporal_stability=0.9,
        window_size=5,
    )

    mid_conf_result = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"WARRIOR": 0.5},
        smoothed_probabilities={"WARRIOR": 0.5},
        dominant_category="WARRIOR",
        dominant_confidence=0.5,
        temporal_stability=0.9,
        window_size=5,
    )

    high_conf_result = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"WARRIOR": 0.8},
        smoothed_probabilities={"WARRIOR": 0.8},
        dominant_category="WARRIOR",
        dominant_confidence=0.8,
        temporal_stability=0.9,
        window_size=5,
    )

    # Test is_valid (min_confidence threshold)
    low_valid = thresholds.is_valid(low_conf_result)
    mid_valid = thresholds.is_valid(mid_conf_result)
    high_valid = thresholds.is_valid(high_conf_result)

    print(f"  Thresholds: min={thresholds.min_confidence}, high={thresholds.high_confidence}")
    print(f"  Validation: low={low_valid} (expect False), mid={mid_valid} (expect True), high={high_valid} (expect True)")

    assert low_valid is False, f"Expected False for conf=0.2 < min=0.3"  # 0.2 < 0.3
    assert mid_valid is True, f"Expected True for conf=0.5 >= min=0.3"   # 0.5 >= 0.3
    assert high_valid is True, f"Expected True for conf=0.8 >= min=0.3"  # 0.8 >= 0.3

    # Test is_high_confidence (weighted threshold)
    # Weighted = conf * (1 - stability_weight) + stability * stability_weight
    # low: 0.2 * 0.7 + 0.9 * 0.3 = 0.14 + 0.27 = 0.41 < 0.7
    # mid: 0.5 * 0.7 + 0.9 * 0.3 = 0.35 + 0.27 = 0.62 < 0.7
    # high: 0.8 * 0.7 + 0.9 * 0.3 = 0.56 + 0.27 = 0.83 >= 0.7
    low_high = thresholds.is_high_confidence(low_conf_result)
    mid_high = thresholds.is_high_confidence(mid_conf_result)
    high_high = thresholds.is_high_confidence(high_conf_result)

    print(f"  High confidence: low={low_high} (expect False), mid={mid_high} (expect False), high={high_high} (expect True)")

    assert low_high is False, f"Expected False for weighted=0.41 < 0.7"
    assert mid_high is False, f"Expected False for weighted=0.62 < 0.7"
    assert high_high is True, f"Expected True for weighted=0.83 >= 0.7"
    return {"tokens": 0}


async def test_transition_event_dataclass():
    """Test TransitionEvent dataclass and serialization."""
    event = TransitionEvent(
        timestamp=1706270400.0,
        from_category="WARRIOR",
        to_category="SAGE",
        from_confidence=0.75,
        to_confidence=0.82,
        stability_at_transition=0.65,
        was_stable=True,
    )

    # Verify fields
    assert event.from_category == "WARRIOR"
    assert event.to_category == "SAGE"
    assert abs(event.to_confidence - event.from_confidence - 0.07) < 0.001
    assert event.was_stable is True

    # Test serialization
    data = event.to_dict()
    assert data["from"] == "WARRIOR"
    assert data["to"] == "SAGE"
    assert abs(data["confidence_change"] - 0.07) < 0.001
    assert data["was_stable"] is True
    assert "timestamp" in data

    print(f"  Transition: {event.from_category} -> {event.to_category}")
    print(f"  Confidence change: {data['confidence_change']:+.2%}")
    print(f"  Was stable: {event.was_stable}")
    return {"tokens": 0}


async def test_transition_detector_persistence():
    """Test TransitionDetector with persistence filtering."""
    detector = TransitionDetector(
        stability_threshold=0.6,
        persistence_frames=3,
    )

    # First classification - WARRIOR
    result1 = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"WARRIOR": 0.8},
        smoothed_probabilities={"WARRIOR": 0.8},
        dominant_category="WARRIOR",
        dominant_confidence=0.8,
        temporal_stability=0.9,  # Above stability_threshold
        window_size=5,
    )

    print(f"  Detector config: stability_threshold={detector.stability_threshold}, persistence_frames={detector.persistence_frames}")

    transition = detector.check_transition(result1)
    assert transition is None, "First frame should not trigger transition"

    # Switch to SAGE - frame 1 of persistence
    result2 = StreamingClassificationResult(
        timestamp=time.time(),
        raw_probabilities={"SAGE": 0.75},
        smoothed_probabilities={"SAGE": 0.75},
        dominant_category="SAGE",
        dominant_confidence=0.75,
        temporal_stability=0.7,
        window_size=5,
    )

    transition = detector.check_transition(result2)
    print(f"  After 1st SAGE frame: pending_count={detector._pending_count}, transition={transition is not None}")
    assert transition is None, f"Only 1 frame of SAGE, need {detector.persistence_frames}"

    # SAGE frame 2
    transition = detector.check_transition(result2)
    print(f"  After 2nd SAGE frame: pending_count={detector._pending_count}, transition={transition is not None}")
    assert transition is None, f"Only 2 frames of SAGE, need {detector.persistence_frames}"

    # SAGE frame 3 - should trigger transition
    transition = detector.check_transition(result2)
    print(f"  After 3rd SAGE frame: pending_count={detector._pending_count}, transition={transition is not None}")
    assert transition is not None, f"3rd frame should trigger (persistence={detector.persistence_frames})"
    assert transition.from_category == "WARRIOR", f"Expected from WARRIOR, got {transition.from_category}"
    assert transition.to_category == "SAGE", f"Expected to SAGE, got {transition.to_category}"
    assert transition.was_stable is True, f"WARRIOR was stable, expected was_stable=True"

    print(f"  Detected transition: {transition.from_category} -> {transition.to_category}")
    print(f"  Previous was stable: {transition.was_stable}")
    return {"tokens": 0}


async def test_adaptive_smoother():
    """Test AdaptiveSmoother dynamic factor adjustment."""
    smoother = AdaptiveSmoother(
        base_factor=0.85,
        min_factor=0.5,
        max_factor=0.95,
        adaptation_rate=0.1,
    )

    # Initial factor should be base
    assert abs(smoother.current_factor - 0.85) < 0.001

    # Low stability -> factor should decrease toward min
    low_factor = smoother.current_factor
    for _ in range(10):
        low_factor = smoother.adapt(stability=0.2)

    # Factor should have decreased (more responsive)
    assert low_factor < 0.85
    assert low_factor >= smoother.min_factor

    print(f"  After low stability: factor={low_factor:.3f}")

    # Reset and test high stability
    smoother.reset()
    assert abs(smoother.current_factor - 0.85) < 0.001

    high_factor = smoother.current_factor
    for _ in range(10):
        high_factor = smoother.adapt(stability=0.95)

    # Factor should have increased (more stable output)
    assert high_factor > 0.85
    assert high_factor <= smoother.max_factor

    print(f"  After high stability: factor={high_factor:.3f}")
    print(f"  Bounds: [{smoother.min_factor}, {smoother.max_factor}]")
    return {"tokens": 0}


async def test_enhanced_classification_result():
    """Test EnhancedClassificationResult dataclass and delegation."""
    base_result = StreamingClassificationResult(
        timestamp=1706270400.0,
        raw_probabilities={"WARRIOR": 0.6, "SAGE": 0.3, "JESTER": 0.1},
        smoothed_probabilities={"WARRIOR": 0.55, "SAGE": 0.35, "JESTER": 0.1},
        dominant_category="WARRIOR",
        dominant_confidence=0.55,
        temporal_stability=0.85,
        window_size=5,
    )

    transition = TransitionEvent(
        timestamp=1706270400.0,
        from_category="SAGE",
        to_category="WARRIOR",
        from_confidence=0.6,
        to_confidence=0.55,
        stability_at_transition=0.8,
        was_stable=True,
    )

    enhanced = EnhancedClassificationResult(
        base_result=base_result,
        is_valid=True,
        is_high_confidence=False,
        transition=transition,
        current_smoothing_factor=0.87,
        latency_ms=12.5,
    )

    # Test delegated properties
    assert enhanced.dominant_category == "WARRIOR"
    assert abs(enhanced.dominant_confidence - 0.55) < 0.001
    assert abs(enhanced.temporal_stability - 0.85) < 0.001

    # Test enhanced fields
    assert enhanced.is_valid is True
    assert enhanced.is_high_confidence is False
    assert enhanced.transition is not None
    assert abs(enhanced.current_smoothing_factor - 0.87) < 0.001
    assert abs(enhanced.latency_ms - 12.5) < 0.001

    # Test serialization
    data = enhanced.to_dict()
    assert data["dominant"] == "WARRIOR"
    assert data["valid"] is True
    assert data["high_confidence"] is False
    assert data["transition"] is not None
    assert "smoothing_factor" in data
    assert "latency_ms" in data

    print(f"  Dominant: {enhanced.dominant_category} ({enhanced.dominant_confidence:.2%})")
    print(f"  Valid: {enhanced.is_valid}, High confidence: {enhanced.is_high_confidence}")
    print(f"  Transition: {enhanced.transition.from_category} -> {enhanced.transition.to_category}")
    return {"tokens": 0}


# =============================================================================
# V39.13 Phase 2: Enhanced Classifier Integration Tests
# =============================================================================

async def test_enhanced_classifier_initialization():
    """Test EnhancedStreamingClassifier initialization with real API."""
    config = EmbeddingConfig(
        cache_enabled=True,
        model=EmbeddingModel.VOYAGE_4_LARGE.value,
    )
    layer = EmbeddingLayer(config)
    await layer.initialize()

    library = ArchetypeEmbeddingLibrary(layer, model=EmbeddingModel.VOYAGE_4_LARGE)
    await library.initialize()

    # Create enhanced classifier with all features
    classifier = EnhancedStreamingClassifier(
        library=library,
        window_size=5,
        base_smoothing_factor=0.85,
        thresholds=ClassificationThresholds(min_confidence=0.3, high_confidence=0.7),
        enable_transition_detection=True,
        enable_adaptive_smoothing=True,
        enable_metrics=True,
    )

    # Verify components are initialized
    assert classifier.window_size == 5
    assert classifier.thresholds is not None
    assert classifier.transition_detector is not None
    assert classifier.adaptive_smoother is not None
    assert classifier.metrics is not None

    print(f"  Window size: {classifier.window_size}")
    print(f"  Thresholds: min={classifier.thresholds.min_confidence}, high={classifier.thresholds.high_confidence}")
    print(f"  Transition detection: enabled")
    print(f"  Adaptive smoothing: enabled")
    print(f"  Metrics tracking: enabled")

    return {"tokens": 2000}


async def test_enhanced_classifier_single_classification():
    """Test single classification with EnhancedStreamingClassifier."""
    config = EmbeddingConfig(
        cache_enabled=True,
        model=EmbeddingModel.VOYAGE_4_LARGE.value,
    )
    layer = EmbeddingLayer(config)
    await layer.initialize()

    library = ArchetypeEmbeddingLibrary(layer, model=EmbeddingModel.VOYAGE_4_LARGE)
    await library.initialize()

    classifier = EnhancedStreamingClassifier(
        library=library,
        window_size=5,
        thresholds=ClassificationThresholds(min_confidence=0.2, high_confidence=0.6),
        enable_transition_detection=True,
        enable_adaptive_smoothing=True,
        enable_metrics=True,
    )

    # Generate test embedding
    test_text = "Person in aggressive fighting stance with fists raised"
    embed_result = await layer.embed([test_text])
    test_embedding = embed_result.embeddings[0]

    # Classify
    result = await classifier.classify(test_embedding)

    assert isinstance(result, EnhancedClassificationResult)
    assert result.dominant_category in ARCHETYPE_NAMES
    assert 0 <= result.dominant_confidence <= 1
    assert isinstance(result.is_valid, bool)
    assert isinstance(result.is_high_confidence, bool)
    assert result.latency_ms > 0

    # Verify metrics were recorded
    assert classifier.metrics.total_classifications == 1

    print(f"  Dominant: {result.dominant_category} ({result.dominant_confidence:.2%})")
    print(f"  Valid: {result.is_valid}, High conf: {result.is_high_confidence}")
    print(f"  Smoothing factor: {result.current_smoothing_factor:.3f}")
    print(f"  Latency: {result.latency_ms:.2f}ms")

    return {"tokens": 2050}


async def test_enhanced_classifier_with_transitions():
    """Test EnhancedStreamingClassifier transition detection."""
    config = EmbeddingConfig(
        cache_enabled=True,
        model=EmbeddingModel.VOYAGE_4_LARGE.value,
    )
    layer = EmbeddingLayer(config)
    await layer.initialize()

    library = ArchetypeEmbeddingLibrary(layer, model=EmbeddingModel.VOYAGE_4_LARGE)
    await library.initialize()

    classifier = EnhancedStreamingClassifier(
        library=library,
        window_size=5,
        enable_transition_detection=True,
        enable_metrics=True,
    )

    # Generate contrasting embeddings
    test_texts = [
        "Aggressive warrior stance with raised fists",
        "Aggressive warrior stance with raised fists",
        "Aggressive warrior stance with raised fists",
        "Calm meditation pose in peaceful contemplation",
        "Calm meditation pose in peaceful contemplation",
        "Calm meditation pose in peaceful contemplation",
    ]

    transitions_found = []
    for text in test_texts:
        embed_result = await layer.embed([text])
        result = await classifier.classify(embed_result.embeddings[0])
        if result.transition:
            transitions_found.append(result.transition)

    # Should detect at least one transition between warrior and sage-like poses
    print(f"  Classifications: {len(test_texts)}")
    print(f"  Transitions detected: {len(transitions_found)}")
    print(f"  Metrics transitions: {classifier.metrics.transitions_detected}")

    for t in transitions_found:
        print(f"    {t.from_category} -> {t.to_category} (conf change: {t.to_confidence - t.from_confidence:+.2%})")

    return {"tokens": 2300}


async def test_enhanced_classifier_metrics_tracking():
    """Test that EnhancedStreamingClassifier properly tracks metrics."""
    config = EmbeddingConfig(
        cache_enabled=True,
        model=EmbeddingModel.VOYAGE_4_LARGE.value,
    )
    layer = EmbeddingLayer(config)
    await layer.initialize()

    library = ArchetypeEmbeddingLibrary(layer, model=EmbeddingModel.VOYAGE_4_LARGE)
    await library.initialize()

    classifier = EnhancedStreamingClassifier(
        library=library,
        window_size=5,
        thresholds=ClassificationThresholds(min_confidence=0.2, high_confidence=0.6),
        enable_metrics=True,
    )

    # Run multiple classifications
    test_texts = [
        "Warrior stance ready for battle",
        "Wise sage in contemplation",
        "Playful jester dancing",
        "Nurturing embrace",
        "Mysterious magician casting",
    ]

    for text in test_texts:
        embed_result = await layer.embed([text])
        await classifier.classify(embed_result.embeddings[0])

    # Verify metrics
    metrics = classifier.metrics
    assert metrics.total_classifications == 5
    assert metrics.avg_latency_ms > 0
    assert len(metrics.category_distribution) > 0

    # Get serialized metrics
    data = metrics.to_dict()
    assert data["total"] == 5
    assert "avg_latency_ms" in data
    assert "distribution" in data

    print(f"  Total classifications: {metrics.total_classifications}")
    print(f"  High confidence count: {metrics.high_confidence_count}")
    print(f"  Avg latency: {metrics.avg_latency_ms:.2f}ms")
    print(f"  Distribution: {metrics.category_distribution}")

    return {"tokens": 2250}


async def test_enhanced_classifier_adaptive_smoothing():
    """Test adaptive smoothing behavior in EnhancedStreamingClassifier."""
    config = EmbeddingConfig(
        cache_enabled=True,
        model=EmbeddingModel.VOYAGE_4_LARGE.value,
    )
    layer = EmbeddingLayer(config)
    await layer.initialize()

    library = ArchetypeEmbeddingLibrary(layer, model=EmbeddingModel.VOYAGE_4_LARGE)
    await library.initialize()

    classifier = EnhancedStreamingClassifier(
        library=library,
        window_size=5,
        base_smoothing_factor=0.85,
        enable_adaptive_smoothing=True,
    )

    initial_factor = classifier.current_smoothing_factor
    assert abs(initial_factor - 0.85) < 0.001

    # Run classifications - smoothing factor should adapt
    smoothing_factors = [initial_factor]
    for i in range(5):
        test_embedding = [float(i) / 10.0] * 1024
        result = await classifier.classify(test_embedding)
        smoothing_factors.append(result.current_smoothing_factor)

    # Factor should have changed from base
    final_factor = classifier.current_smoothing_factor

    print(f"  Initial factor: {initial_factor:.3f}")
    print(f"  Final factor: {final_factor:.3f}")
    print(f"  Factor progression: {[f'{f:.3f}' for f in smoothing_factors]}")
    print(f"  Adaptation occurred: {initial_factor != final_factor}")

    return {"tokens": 2000}


async def test_enhanced_classifier_serialization():
    """Test EnhancedClassificationResult serialization for protocols."""
    config = EmbeddingConfig(
        cache_enabled=True,
        model=EmbeddingModel.VOYAGE_4_LARGE.value,
    )
    layer = EmbeddingLayer(config)
    await layer.initialize()

    library = ArchetypeEmbeddingLibrary(layer, model=EmbeddingModel.VOYAGE_4_LARGE)
    await library.initialize()

    classifier = EnhancedStreamingClassifier(
        library=library,
        window_size=5,
        thresholds=ClassificationThresholds(min_confidence=0.2, high_confidence=0.6),
        enable_transition_detection=True,
        enable_adaptive_smoothing=True,
        enable_metrics=True,
    )

    # Generate real classification
    test_text = "Contemplative meditation pose"
    embed_result = await layer.embed([test_text])
    result = await classifier.classify(embed_result.embeddings[0])

    # Serialize
    data = result.to_dict()

    # Verify all required fields for protocol
    required_fields = [
        "timestamp", "raw", "smoothed", "dominant", "confidence",
        "stability", "window", "valid", "high_confidence",
        "smoothing_factor", "latency_ms", "transition"
    ]

    for field in required_fields:
        assert field in data, f"Missing field: {field}"

    # Verify types
    assert isinstance(data["timestamp"], float)
    assert isinstance(data["raw"], dict)
    assert isinstance(data["smoothed"], dict)
    assert isinstance(data["dominant"], str)
    assert isinstance(data["confidence"], float)
    assert isinstance(data["valid"], bool)
    assert isinstance(data["high_confidence"], bool)
    assert isinstance(data["latency_ms"], float)

    print(f"  Serialized fields: {len(data)}")
    print(f"  Protocol-ready: all required fields present")
    print(f"  Sample: dominant={data['dominant']}, valid={data['valid']}")

    return {"tokens": 2050}


async def main():
    """Run all V39.13 classification enhancement tests."""
    print("=" * 60)
    print("VOYAGE AI V39.13 - CLASSIFICATION ENHANCEMENT & METRICS TESTS")
    print("=" * 60)
    print("\nNote: These tests make REAL API calls to Voyage AI.")
    print("      Cost estimates are shown for each test.\n")

    start_time = time.time()

    # Phase 1: Core Infrastructure Tests
    print("\n" + "=" * 60)
    print("PHASE 1: Core Infrastructure Tests")
    print("=" * 60)

    await run_test("test_classification_metrics_dataclass", test_classification_metrics_dataclass)
    await run_test("test_classification_metrics_running_averages", test_classification_metrics_running_averages)
    await run_test("test_classification_metrics_serialization", test_classification_metrics_serialization)
    await run_test("test_classification_thresholds", test_classification_thresholds)
    await run_test("test_transition_event_dataclass", test_transition_event_dataclass)
    await run_test("test_transition_detector_persistence", test_transition_detector_persistence)
    await run_test("test_adaptive_smoother", test_adaptive_smoother)
    await run_test("test_enhanced_classification_result", test_enhanced_classification_result)

    # Phase 2: Enhanced Classifier Integration Tests
    print("\n" + "=" * 60)
    print("PHASE 2: Enhanced Classifier Integration Tests")
    print("=" * 60)

    await run_test("test_enhanced_classifier_initialization", test_enhanced_classifier_initialization)
    await run_test("test_enhanced_classifier_single_classification", test_enhanced_classifier_single_classification)
    await run_test("test_enhanced_classifier_with_transitions", test_enhanced_classifier_with_transitions)
    await run_test("test_enhanced_classifier_metrics_tracking", test_enhanced_classifier_metrics_tracking)
    await run_test("test_enhanced_classifier_adaptive_smoothing", test_enhanced_classifier_adaptive_smoothing)
    await run_test("test_enhanced_classifier_serialization", test_enhanced_classifier_serialization)

    elapsed = time.time() - start_time

    # Summary
    print("\n" + "=" * 60)
    print(f"RESULTS: {TEST_RESULTS['passed']} passed, {TEST_RESULTS['failed']} failed, {TEST_RESULTS['skipped']} skipped")
    print("=" * 60)

    print("\n" + "=" * 60)
    print("COST SUMMARY")
    print("=" * 60)
    print(f"  Tests run: {TEST_RESULTS['passed'] + TEST_RESULTS['failed']}")
    print(f"  Tests skipped: {TEST_RESULTS['skipped']}")
    print(f"  Total tokens: {TEST_RESULTS['total_tokens']:,}")
    print(f"  Total cost: ${TEST_RESULTS['total_cost']:.4f}")
    print(f"  Duration: {elapsed:.1f}s")
    print("=" * 60)

    if TEST_RESULTS["failed"] == 0:
        print("\n[SUCCESS] V39.13 CLASSIFICATION ENHANCEMENT TESTS COMPLETE!")
        print("          Metrics, thresholds, transitions, and adaptive smoothing validated.")
    else:
        print(f"\n[FAILURE] {TEST_RESULTS['failed']} test(s) failed!")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
