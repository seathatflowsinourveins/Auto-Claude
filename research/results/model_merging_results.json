{
  "timestamp": "2026-02-03T02:50:47.540978",
  "stats": {
    "sources": 200,
    "vectors": 200,
    "findings": 100
  },
  "results": [
    {
      "topic": "Model merging: weight averaging",
      "area": "methods",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Machine Learning",
        "[exa] Computer Science > Machine Learning",
        "[exa-h] > Model merging, particularly through weight averaging, has shown surprising effectiveness in saving computations and im"
      ],
      "latency": 6.944015264511108
    },
    {
      "topic": "TIES merging: trim elect sign",
      "area": "methods",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] TE Connectivity powers down high-power systems",
        "[exa] \u201cSquib\u201d Driver IC Helps Ensure Rapid EV/HEV Battery-Pack Cutoff",
        "[exa-h] _Jon Harman, Global Vice President, Sales and Customer Care, TE Connectivity, wrote this article for SAE Media. He previ"
      ],
      "latency": 8.083512306213379
    },
    {
      "topic": "DARE: delta parameter rescaling",
      "area": "methods",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] ",
        "[exa-h] but recent LLM architectures often omit bias terms for training stability (Touvron et al., 2023; Chowdhery et al., 2023)"
      ],
      "latency": 6.616854429244995
    },
    {
      "topic": "SLERP: spherical interpolation",
      "area": "methods",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Slerp",
        "[exa] Slerping Clock Cycles",
        "[exa-h] Quaternion slerps are commonly used to construct smooth animation curves by mimicking affine constructions like the [deC"
      ],
      "latency": 7.860591888427734
    },
    {
      "topic": "Task vectors: model arithmetic",
      "area": "techniques",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Computation and Language",
        "[exa] Computer Science > Machine Learning",
        "[exa-h] > Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning t"
      ],
      "latency": 5.928002595901489
    },
    {
      "topic": "Linear interpolation: weight blending",
      "area": "techniques",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Linear interpolation",
        "[exa] 1  Basic lerp",
        "[exa-h] This lerp function is commonly used for [alpha blending] (the parameter \"t\" is the \"alpha value\"), and the formula may b"
      ],
      "latency": 6.987936735153198
    },
    {
      "topic": "Frankenmerging: layer mixing",
      "area": "techniques",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Machine Learning",
        "[exa] Computer Science > Computation and Language",
        "[exa-h] > Model merging offers an efficient alternative to multi-task learning by combining independently fine-tuned models, but"
      ],
      "latency": 7.0348591804504395
    },
    {
      "topic": "Model soup: multi-model averaging",
      "area": "techniques",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] Model soups: averaging weights of multiple fine-tuned models improves\n  accuracy without increasing inference time",
        "[exa-h] [We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.]"
      ],
      "latency": 8.519745111465454
    },
    {
      "topic": "mergekit: merging toolkit",
      "area": "tools",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Search code, repositories, users, issues, pull requests...",
        "[exa] Computer Science > Computation and Language",
        "[exa-h] `mergekit`is a toolkit for merging pre-trained language models.`mergekit`uses an out-of-core approach to perform unreaso"
      ],
      "latency": 5.722353219985962
    },
    {
      "topic": "LM-Cocktail: mixture recipes",
      "area": "tools",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Batched 50-50 Martini",
        "[exa] Manhattan Recipe",
        "[exa-h] - 9ounces dry gin\n- 9ounces dry vermouth\n- 4\u00bdounces filtered water\n- Lemon twists, green olives, olive brine, cocktail o"
      ],
      "latency": 8.1039879322052
    },
    {
      "topic": "MergeBoard: leaderboard",
      "area": "tools",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] December 2025 Season Leaderboard | Top 10,000 Players - Merge Tactics | Merge Tactics GG",
        "[exa] The Best  Free Leaderboard Maker",
        "[exa-h] Track the **top 10,000 players** and monitor competitive standings in real-time. **Click any player name** to view their"
      ],
      "latency": 6.24365234375
    },
    {
      "topic": "Hugging Face merging: hosted tools",
      "area": "tools",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Model merging",
        "[exa] ",
        "[exa-h] Training a model for each task can be costly, take up storage space, and the models aren\u2019t able to learn new information"
      ],
      "latency": 7.157333850860596
    },
    {
      "topic": "Specialized merges: domain experts",
      "area": "applications",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Computation and Language",
        "[exa] Not all who integrate are academics: zooming in on extra-academic integrative expertise",
        "[exa-h] > We present Branch-Train-Stitch (BTS), an efficient and flexible training algorithm for combining independently trained"
      ],
      "latency": 8.659801006317139
    },
    {
      "topic": "Multilingual merges: language transfer",
      "area": "applications",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] ",
        "[exa-h] and language mixing. In this chapter, language transfer and cross-linguistic influence\nwill be used interchangeably, as "
      ],
      "latency": 7.074673414230347
    },
    {
      "topic": "Code merges: programming models",
      "area": "applications",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] ",
        "[exa-h] To address this problem, we introduce MergeBERT, a novel neural\nprogram merge framework based on token-level three-way d"
      ],
      "latency": 11.027950763702393
    },
    {
      "topic": "Chat merges: conversational",
      "area": "applications",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Conversation state",
        "[exa] Chat Completions",
        "[exa-h] # Conversation state\nLearn how to manage conversation state during a model interaction.\nCopy page\nOpenAI provides a few "
      ],
      "latency": 8.853325843811035
    },
    {
      "topic": "Model interpolation: theory",
      "area": "research",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Statistics > Machine Learning",
        "[exa] Interpolation theory",
        "[exa-h] efforts to understand the foundations of deep learning. The two key themes will be interpolation, and its sibling, over-"
      ],
      "latency": 10.977181673049927
    },
    {
      "topic": "Loss landscape: flat minima",
      "area": "research",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Machine Learning",
        "[exa] Optimization Landscape and Expressivity of Deep CNNs",
        "[exa-h] > Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is wel"
      ],
      "latency": 8.03499984741211
    },
    {
      "topic": "Mode connectivity: neural paths",
      "area": "research",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Brain network communication: concepts, models and applications",
        "[exa] Whole-animal connectomes of both  Caenorhabditis elegans  sexes",
        "[exa-h] Understanding communication and information processing in nervous systems is a central goal of neuroscience. Over the pa"
      ],
      "latency": 6.443363189697266
    },
    {
      "topic": "Ensemble distillation: combination",
      "area": "research",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Statistics > Machine Learning",
        "[exa] Computer Science > Machine Learning",
        "[exa-h] work considers the novel task of \\emph{Ensemble Distribution Distillation} (EnD$^2$) --- distilling the distribution of "
      ],
      "latency": 7.728334665298462
    }
  ]
}