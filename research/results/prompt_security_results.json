{
  "timestamp": "2026-02-03T01:51:22.142606",
  "stats": {
    "sources": 200,
    "vectors": 190,
    "findings": 87
  },
  "results": [
    {
      "topic": "Prompt injection attacks: direct, indirect, nested",
      "area": "injection",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Cryptography and Security",
        "[exa] Computer Science > Computation and Language",
        "[exa-h] > Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recen"
      ],
      "latency": 8.998843431472778
    },
    {
      "topic": "Indirect prompt injection: document, email, web",
      "area": "injection",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] Indirect prompt injections & Google's layered defense strategy for Gemini",
        "[exa-h] or documents and to access programming environments or mailboxes. In many of the anticipated use cases, \nunverified data"
      ],
      "latency": 6.637615919113159
    },
    {
      "topic": "Prompt injection defense: detection, prevention",
      "area": "injection",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Cryptography and Security",
        "[exa] Computer Science > Cryptography and Security",
        "[exa-h] > Application designers have moved to integrate large language models (LLMs) into their products. However, many LLM-inte"
      ],
      "latency": 8.492323160171509
    },
    {
      "topic": "Input sanitization: filtering malicious prompts",
      "area": "injection",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Input Validation Cheat Sheet \u00b6",
        "[exa] Input Validation",
        "[exa-h] ## Validating Rich User Content[\u00b6] \nIt is very difficult to validate rich content submitted by a user. For more informat"
      ],
      "latency": 6.830541610717773
    },
    {
      "topic": "Jailbreak techniques: DAN, roleplay, encoding",
      "area": "jailbreak",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ChatGPT Jailbreak",
        "[exa] Understanding AI Jailbreaking: Techniques and Safeguards Against Prompt Exploits",
        "[exa-h] Finally, I will show you a couple of commands that I can use in the chat. If I start any of my messages with these comma"
      ],
      "latency": 4.731881856918335
    },
    {
      "topic": "Jailbreak detection: pattern matching, classifiers",
      "area": "jailbreak",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Computation and Language",
        "[exa] Computer Science > Cryptography and Security",
        "[exa-h] > Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass mo"
      ],
      "latency": 5.18933629989624
    },
    {
      "topic": "Universal adversarial prompts: transferable attacks",
      "area": "jailbreak",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Universal and Transferable Adversarial Attacks on Aligned Language Models",
        "[exa] ",
        "[exa-h] Surprisingly, we find that the adversarial prompts generated by our approach are highly _transferable_, including to bla"
      ],
      "latency": 7.903640270233154
    },
    {
      "topic": "Jailbreak mitigation: system prompts, fine-tuning",
      "area": "jailbreak",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Configuration enforcement",
        "[exa] ",
        "[exa-h] Configurations are the primary way that a device management service delivers and manages policies and restrictions on ma"
      ],
      "latency": 4.013843536376953
    },
    {
      "topic": "Training data extraction: memorization attacks",
      "area": "data",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Machine Learning",
        "[exa] ",
        "[exa-h] > This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a mac"
      ],
      "latency": 4.6353089809417725
    },
    {
      "topic": "PII leakage prevention: output filtering",
      "area": "data",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Handling sensitive data",
        "[exa] Protect PII, PHI, and other sensitive data with field filters",
        "[exa-h] The[OpenTelemetry Collector] provides several processors that\ncan help manage sensitive data:\n* [`attribute`processor]:\n"
      ],
      "latency": 5.60456395149231
    },
    {
      "topic": "Membership inference: detecting training data",
      "area": "data",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] ",
        "[exa-h] which they were trained. We focus on the basic membership\ninference attack: given a data record and black-box access to\n"
      ],
      "latency": 3.9205238819122314
    },
    {
      "topic": "Data poisoning: training data attacks",
      "area": "data",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks - ADS",
        "[exa] Computer Science > Machine Learning",
        "[exa-h] Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipul"
      ],
      "latency": 7.127284049987793
    },
    {
      "topic": "Adversarial examples for LLMs: perturbations",
      "area": "adversarial",
      "sources": 10,
      "vectors": 0,
      "findings": [
        "[exa] Computer Science > Computation and Language",
        "[exa] Computer Science > Computation and Language",
        "[exa-h] > Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent"
      ],
      "latency": 3.8483359813690186
    },
    {
      "topic": "Model stealing: extracting model capabilities",
      "area": "adversarial",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Stealing Part of a Production Language Model",
        "[exa] ",
        "[exa-h] In this paper we ask: how much information can an adver\u0002sary learn about a production language model by making\nqueries t"
      ],
      "latency": 5.199950933456421
    },
    {
      "topic": "Backdoor attacks: hidden model behaviors",
      "area": "adversarial",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Cryptography and Security",
        "[exa] ",
        "[exa-h] > Abstract:The rapid proliferation of open-source language models significantly increases the risks of downstream backdo"
      ],
      "latency": 5.440691709518433
    },
    {
      "topic": "Robustness evaluation: red team testing",
      "area": "adversarial",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] ",
        "[exa-h] Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\ndefines the term AI "
      ],
      "latency": 4.200072765350342
    },
    {
      "topic": "Defense in depth: layered security for LLMs",
      "area": "defense",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Security planning for LLM-based applications",
        "[exa] A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks",
        "[exa-h] have robust defense mechanisms implemented using SIEM and SOC tools."
      ],
      "latency": 3.7781143188476562
    },
    {
      "topic": "Constitutional AI: self-correcting safety",
      "area": "defense",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] ",
        "[exa] ",
        "[exa-h] other AIs. We experiment with methods for training a harmless AI assistant through self\u0002improvement, without any human l"
      ],
      "latency": 5.441356658935547
    },
    {
      "topic": "Sandboxing LLM outputs: isolated execution",
      "area": "defense",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Computer Science > Cryptography and Security",
        "[exa] Search code, repositories, users, issues, pull requests...",
        "[exa-h] paper, we evaluate whether these issues can be addressed through execution isolation and what that isolation might look "
      ],
      "latency": 9.172654151916504
    },
    {
      "topic": "Security monitoring: detecting attacks in production",
      "area": "defense",
      "sources": 10,
      "vectors": 10,
      "findings": [
        "[exa] Architecture strategies for monitoring and threat detection",
        "[exa] Microsoft Sentinel documentation",
        "[exa-h] This guide describes the recommendations for monitoring and threat detection. Monitoring is fundamentally a process of**"
      ],
      "latency": 9.578331708908081
    }
  ]
}